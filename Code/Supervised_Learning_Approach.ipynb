{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import operator \n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "from datetime import date\n",
    "import operator\n",
    "from matplotlib import pyplot\n",
    "import rpy2 as r\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the files in the same folder with code\n",
    "confirmed_cases=pd.read_csv('confirmed_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_cases=pd.read_csv('deaths_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoveries_cases=pd.read_csv('recoveries_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = confirmed_cases.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonames = recoveries_cases.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = confirmed_cases.loc[:, colnames[4]:colnames[-1]]\n",
    "deaths = deaths_cases.loc[:, colnames[4]:colnames[-1]]\n",
    "recoveries = recoveries_cases.loc[:, colonames[4]:colonames[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_date = confirmed.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = np.array([i for i in range(len(time_date))]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_days_ahead = 2\n",
    "#pred_days_ahead = 7\n",
    "pred_days_ahead = 30\n",
    "predictions_ahead = np.array([i for i in range(len(time_date)+pred_days_ahead)]).reshape(-1, 1)\n",
    "\n",
    "predictions_ahead_plus_day = np.array([i for i in range(len(time_date)+pred_days_ahead+1)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.strptime(colnames[4], \"%m/%d/%y\")\n",
    "end = datetime.datetime.strptime(colnames[-1], \"%m/%d/%y\")\n",
    "predictions_ahead_dates = [start + datetime.timedelta(days=d) for d in range(len(predictions_ahead_plus_day))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries=confirmed_cases['Country/Region'].tolist()\n",
    "province=confirmed_cases['Province/State'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_cases = confirmed_cases[~confirmed_cases['Country/Region'].isin(['Tajikistan', 'Comoros'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_stationarity(timeseries):\n",
    "    \n",
    "#     #Determing rolling statistics\n",
    "#     rolmean = timeseries.rolling(12).mean()     \n",
    "#     rolstd = timeseries.rolling(12).std()\n",
    "    \n",
    "#     #Perform Dickey-Fuller test:\n",
    "#     dftest = adfuller(timeseries, autolag='AIC')\n",
    "#     dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "#     for key,value in dftest[4].items():\n",
    "#         dfoutput['Critical Value (%s)'%key] = value\n",
    "#     return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(last_ob, value):\n",
    "    return value + last_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input timeseries as a dataframe\n",
    "# def make_stationary(timeseries):\n",
    "#     #remove zeroes to find the optimal d\n",
    "#     timeseries=timeseries[timeseries['cases'] != 0]\n",
    "#     #find the optimal d \n",
    "#     diff=timeseries-timeseries.shift(1)\n",
    "#     for d in range(1,10):\n",
    "#         if (test_stationarity(diff.dropna())['p-value']<0.1):\n",
    "#             break\n",
    "#         else: \n",
    "#             d=d+1 \n",
    "#             diff=diff-diff.shift(1)\n",
    "#     return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['diff'+str(i)]=df['diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takensEmbedding (data, delay, dimension):\n",
    "    \"This function returns the Takens embedding of data with delay into dimension, delay*dimension must be < len(data)\"\n",
    "    if delay*dimension > len(data):\n",
    "        raise NameError('Delay times dimension exceed length of data!')    \n",
    "    embeddedData = np.array([data[0:len(data)-delay*dimension]])\n",
    "    for i in range(1, dimension):\n",
    "        embeddedData = np.append(embeddedData, [data[i*delay:len(data) - delay*(dimension - i)]], axis=0)\n",
    "    return embeddedData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data (region, delay=1 , dimension=31):\n",
    "    #create a dataframe for timeseries analysis\n",
    "    date_rng = pd.date_range(start=colnames[4], end=colnames[-1], freq='D') #creates the days\n",
    "    df = pd.DataFrame(date_rng, columns=['date'])\n",
    "    df['cases']=np.transpose(region) #this is a dataframe with dates and cases.\n",
    "#     df = df.set_index('date')\n",
    "#     timeseries=df[df['cases'] != 0]\n",
    "#     #find the optimal d \n",
    "#     diff=timeseries-timeseries.shift(1)\n",
    "#     for d in range(1,10):\n",
    "#         if (test_stationarity(diff.dropna())['p-value']<0.1):\n",
    "#             break\n",
    "#         else: \n",
    "#             d=d+1 \n",
    "#             diff=diff-diff.shift(1)\n",
    "#     for i in range (1,d):\n",
    "#         i=i+1 \n",
    "#         diff=diff-diff.shift(1)\n",
    "#         df['diff'+str(i)]=diff\n",
    "    #df['diff']=diff #column of the dataframe with the stationary timeseries, might contain Na's!\n",
    "    df['diff']=df['cases']-df['cases'].shift(1)\n",
    "    t_diff=(df['diff'].dropna()).to_numpy() \n",
    "    timeseries_embedded=np.transpose(takensEmbedding(t_diff,delay,dimension))\n",
    "    Y_train=timeseries_embedded[:,-1]\n",
    "    X_train=timeseries_embedded[:,:-1] #keep all apart from the last column\n",
    "    X_test=t_diff[-(dimension-1):] #predict using as feature the last 'dimension' lags\n",
    "    return X_train , Y_train, X_test , t_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data(reg,1,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=prepare_data(reg,1,31)[0]\n",
    "# Y_train=prepare_data(reg,1,31)[1]\n",
    "# X_test=prepare_data(reg,1,31)[2]\n",
    "# t_diff=prepare_data(reg,1,31)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=t_diff[-30:]\n",
    "# X_test\n",
    "# pred_days_ahead=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "#     X_test=np.append(X_test[1:],predictions[i])\n",
    "#     #in every iteration remove the first element of X_test and add prediction\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together=np.append(t_diff,predictions)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together=np.cumsum(together)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,d):\n",
    "#         together=np.cumsum(together)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_ML(region, pred_days_ahead, delay=1,dimension=30):\n",
    "    predictions=np.zeros(pred_days_ahead)\n",
    "    X_train = prepare_data(region, delay, dimension)[0] \n",
    "    Y_train = prepare_data(region, delay, dimension)[1]\n",
    "    X_test = prepare_data(region, delay, dimension)[2]\n",
    "    for i in range(pred_days_ahead):\n",
    "        regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "        r=regressor.fit(X_train, Y_train)\n",
    "        predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "        X_test=np.append(X_test[1:],int(predictions[i])) \n",
    "        #in every iteration remove the first element of X_test and add prediction\n",
    "    predictions #we have to go back to the original scale\n",
    "    #d = prepare_data(region, delay, dimension)[3]\n",
    "    t_diff = prepare_data(region, delay, dimension)[3]\n",
    "    together=np.append(t_diff,predictions)\n",
    "    #for i in range(1,d):\n",
    "    together=np.cumsum(together)\n",
    "    return together #returns an array with all the cases and the predictions (we want the last elements of it)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=predict_with_ML(reg,30,1,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_days_ahead=30\n",
    "# x[-pred_days_ahead+6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = confirmed_cases.loc[(confirmed_cases['Country/Region'] == \"US\") & (pd.isnull(confirmed_cases['Province/State']))]\n",
    "# reg =  np.array(region.loc[:, colnames[4]:colnames[-1]])\n",
    "# len(reg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_rng = pd.date_range(start=colnames[4], end=colnames[-1], freq='D') #creates the days\n",
    "# df = pd.DataFrame(date_rng, columns=['date'])\n",
    "# df['cases']=np.transpose(reg) #this is a dataframe with dates and cases.\n",
    "# df = df.set_index('date')\n",
    "# #df=df[df['cases'] != 0] #remove zeroes, they do not help for analysis.\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['diff']=df['cases']-df['cases'].shift(1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def takensEmbedding (data, delay, dimension):\n",
    "#     \"This function returns the Takens embedding of data with delay into dimension, delay*dimension must be < len(data)\"\n",
    "#     if delay*dimension > len(data):\n",
    "#         raise NameError('Delay times dimension exceed length of data!')    \n",
    "#     embeddedData = np.array([data[0:len(data)-delay*dimension]])\n",
    "#     for i in range(1, dimension):\n",
    "#         embeddedData = np.append(embeddedData, [data[i*delay:len(data) - delay*(dimension - i)]], axis=0)\n",
    "#     return embeddedData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_diff=(df['diff'].dropna()).to_numpy() \n",
    "# np.shape(t_diff) #the function wants as input an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries_embedded=np.transpose(takensEmbedding(t_diff,1,11))\n",
    "# np.shape(timeseries_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train=timeseries_embedded[:,-1]\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=timeseries_embedded[:,:-1] #keep all apart from the last column\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.transpose(reg)[-30:].reshape(1,-1)\n",
    "#X_test=np.transpose(x)[0].reshape(1,-1)\n",
    "#X_test=np.transpose(t_diff[-30:]).reshape(1,-1)\n",
    "# X_test=t_diff[-10:]\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #regressor = RandomForestRegressor(n_estimators = 500, random_state = 42) \n",
    "# regressor =  GradientBoostingRegressor(n_estimators=500, learning_rate=0.1,\n",
    "#                                             max_depth=1, random_state=0, loss='huber')\n",
    "# #regressor = SVR(kernel='rbf', C=1 )\n",
    "# r=regressor.fit(X_train, Y_train)\n",
    "# predictions = regressor.predict(X_test.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "# #X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=np.append(X_test[1:],int(predictions))\n",
    "# #len(X_test)\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_days_ahead=2\n",
    "# X_train\n",
    "# #X_train[:-1,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "#     X_test=np.append(X_test[1:],int(predictions[i])) \n",
    "#     #in every iteration remove the first element of X_test and add prediction\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test)\n",
    "#     Y_train = Y_train[:-1] #remove the last element of Y_train\n",
    "#     X_train=X_train[:-1,:] #remove the last row of X_train\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def subpopulation_Analysis(cases,country,province, all_days,subpopulation, pred_days_ahead=30):\n",
    "    \n",
    "    pred  = list()\n",
    "    if pd.isnull(province):\n",
    "        region = cases.loc[(cases['Country/Region'] == country) & (pd.isnull(cases['Province/State']))]\n",
    "    else :\n",
    "        region = cases.loc[(cases['Country/Region'] == country) & (cases['Province/State'] == province)]\n",
    "    print(region)\n",
    "    \n",
    "    reg =  np.array(region.loc[:, colnames[4]:colnames[-1]])\n",
    "    region =  np.array(region.loc[:, colnames[4]:colnames[-1]]).reshape(-1, 1)\n",
    "        \n",
    "    #train_x, test_x, train_y, test_y = model_selection.train_test_split(all_days,region,test_size = 0.04,random_state = 42,shuffle=False)\n",
    "    \n",
    "    #pred_svm =  SVM_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "    #pred.append(pred_svm)\n",
    "    \n",
    "    #TODO: a for loop\n",
    "    daily_increase_threshold1=np.absolute((reg[-1][-1]-reg[-1][-2])/reg[-1][-2])*100\n",
    "    daily_increase_threshold2=np.absolute((reg[-1][-2]-reg[-1][-3])/reg[-1][-3])*100\n",
    "    daily_increase_threshold3=np.absolute((reg[-1][-3]-reg[-1][-4])/reg[-1][-4])*100\n",
    "    daily_increase_threshold4=np.absolute((reg[-1][-4]-reg[-1][-5])/reg[-1][-5])*100\n",
    "    daily_increase_bound=1000\n",
    "    daily_increase_threshold = max(daily_increase_threshold1,daily_increase_threshold2,daily_increase_threshold3,daily_increase_threshold4)\n",
    "    \n",
    "    if (daily_increase_threshold>daily_increase_bound):\n",
    "        best_pred_ahead2 = 0 \n",
    "        best_pred_ahead7 = 0 \n",
    "        best_pred_ahead30 = 0 \n",
    "        region = region\n",
    "        best_pred = 0\n",
    "        #best_conf_int = np.zeros(2)\n",
    "        reject_country = 1\n",
    "        return(best_pred_ahead2,best_pred_ahead7,best_pred_ahead30, region, best_pred, reject_country)\n",
    "     \n",
    "    \n",
    "    pred_arima=predict_with_ML(reg, pred_days_ahead, delay=1,dimension=31)\n",
    "    #pred.append(pred_svm)\n",
    "    #pred_RF = RF_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "    #pred.append(pred_RF)\n",
    "\n",
    "    \n",
    "   # pred_gb =  Grad_Boost_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "   # pred.append(pred_gb)\n",
    "\n",
    "\n",
    "    if (subpopulation == 'confirmed'): \n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_confirmed.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "    elif (subpopulation == 'deaths'):\n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_deaths.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "    else :\n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_recoveries.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "        \n",
    "    #print(best_pred)\n",
    "    #print(type(best_pred))\n",
    "    best_pred_ahead2 = best_pred[-pred_days_ahead+1]\n",
    "    #print(\"gntm2\")\n",
    "    #print(best_pred_ahead2)\n",
    "    best_pred_ahead7= best_pred[-pred_days_ahead+6]\n",
    "    #print(\"gntm7\")\n",
    "    #print(best_pred_ahead7)\n",
    "    best_pred_ahead30= best_pred[-pred_days_ahead-1]\n",
    "    #print(\"gntm30\")\n",
    "    #print(best_pred_ahead30)\n",
    "    #best_conf_int=conf_int[-1]\n",
    "   \n",
    "    #best_conf_int[0]=max(reg[-1][-1], best_conf_int[0])\n",
    "    #if (best_conf_int[0]>best_conf_int[1]):\n",
    "        #best_conf_int[1]=2*reg[-1][-1]\n",
    "\n",
    "    if(best_pred_ahead2<reg[-1][-1]):\n",
    "        best_pred_ahead2= reg[-1][-1]+2\n",
    "    \n",
    "    if(best_pred_ahead7<reg[-1][-1]):\n",
    "        best_pred_ahead7= reg[-1][-1]+7   \n",
    "    \n",
    "    if(best_pred_ahead7<reg[-1][-1]):\n",
    "        best_pred_ahead7= reg[-1][-1]+30     \n",
    "    \n",
    "    reject_country = 0\n",
    "    \n",
    "    return best_pred_ahead2,best_pred_ahead7,best_pred_ahead30, region, best_pred, reject_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2day_prediction_exp_smooth2020-05-08.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file2 = str(2)+\"day_prediction_random_forest\" +  date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7day_prediction_exp_smooth2020-05-08.csv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file7 = str(7)+\"day_prediction_random_forest\" + date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30day_prediction_exp_smooth2020-05-08.csv'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file30 = str(30)+\"day_prediction_random_forest\" + date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  4/28/20  4/29/20  4/30/20  5/1/20  5/2/20  \\\n",
      "0        0        0        0  ...     1828     1939     2171    2335    2469   \n",
      "\n",
      "   5/3/20  5/4/20  5/5/20  5/6/20  5/7/20  \n",
      "0    2704    2894    3224    3392    3563  \n",
      "\n",
      "[1 rows x 109 columns]\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  4/28/20  4/29/20  4/30/20  5/1/20  5/2/20  \\\n",
      "0        0        0        0  ...       58       60       64      68      72   \n",
      "\n",
      "   5/3/20  5/4/20  5/5/20  5/6/20  5/7/20  \n",
      "0      85      90      95     104     106  \n",
      "\n",
      "[1 rows x 109 columns]\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  4/28/20  4/29/20  4/30/20  5/1/20  5/2/20  \\\n",
      "0        0        0        0  ...      228      252      260     310     331   \n",
      "\n",
      "   5/3/20  5/4/20  5/5/20  5/6/20  5/7/20  \n",
      "0     345     397     421     458     468  \n",
      "\n",
      "[1 rows x 109 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-9ed85e271bc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         best_pred_recoveries2,best_pred_recoveries7,best_pred_recoveries30, region_r, best_pred_r, reject_country = subpopulation_Analysis(recoveries_cases,countries[co],province[co],\n\u001b[1;32m---> 36\u001b[1;33m                                                   all_days,\"recoveries\",pred_days_ahead)\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreject_country\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-cc4defc1a80f>\u001b[0m in \u001b[0;36msubpopulation_Analysis\u001b[1;34m(cases, country, province, all_days, subpopulation, pred_days_ahead)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mpred_arima\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_with_ML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_days_ahead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#pred.append(pred_svm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#pred_RF = RF_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-1ac8894ee1a1>\u001b[0m in \u001b[0;36mpredict_with_ML\u001b[1;34m(region, pred_days_ahead, delay, dimension)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_days_ahead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 383\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns = ['Province/State','Country','Target/Date','N','low95N','high95N',\n",
    "          'R','low95R','high95R','D','low95D','high95D','T','low95T','high95T',\n",
    "           'M','low95M','high95M','C','low95C','high95C']\n",
    "\n",
    "            \n",
    "\n",
    "k = 0\n",
    "for co in range(len(countries)):\n",
    "    \n",
    "    ignore_recoveries = 0    \n",
    "    \n",
    "    total_predictions2 = pd.DataFrame(columns = columns)\n",
    "    total_predictions7 = pd.DataFrame(columns = columns)\n",
    "    total_predictions30 = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    k = k + 1\n",
    "   \n",
    "       \n",
    "    best_pred_confirmed2,best_pred_confirmed7,best_pred_confirmed30, region_con, best_pred_con, reject_country = subpopulation_Analysis(confirmed_cases,countries[co],province[co],\n",
    "                                                 all_days,\"confirmed\",pred_days_ahead)\n",
    "    \n",
    "    if (reject_country==1):\n",
    "        print('rejected_conf')\n",
    "        continue\n",
    "        \n",
    "    best_pred_deaths2,best_pred_deaths7,best_pred_deaths30, region_d, best_pred_d, reject_country = subpopulation_Analysis(deaths_cases,countries[co],province[co],\n",
    "                                              all_days,\"deaths\",pred_days_ahead)\n",
    "    if (reject_country==1):\n",
    "        print('rejected_d')\n",
    "        continue\n",
    "        \n",
    "    if ((countries[co] == 'US' or countries[co] == 'Canada' ) & (pd.notnull(province[co]))):\n",
    "        ignore_recoveries=1   \n",
    "    else:\n",
    "        best_pred_recoveries2,best_pred_recoveries7,best_pred_recoveries30, region_r, best_pred_r, reject_country = subpopulation_Analysis(recoveries_cases,countries[co],province[co],\n",
    "                                                  all_days,\"recoveries\",pred_days_ahead)\n",
    "    \n",
    "    if (reject_country==1):\n",
    "        print('rejected_r')\n",
    "        continue\n",
    "    \n",
    "    # Mortality rate\n",
    "    #if (conf_int_con[0]==0):\n",
    "       # mort_pred=0\n",
    "        #mort_pred_low=0\n",
    "        #mort_pred_up=0\n",
    "    #else:\n",
    "    mort_pred2  = best_pred_deaths2/best_pred_confirmed2\n",
    "    mort_pred7  = best_pred_deaths7/best_pred_confirmed7\n",
    "    mort_pred30  = best_pred_deaths30/best_pred_confirmed30\n",
    "        #mort_pred_low= conf_int_d[0]/conf_int_con[1]\n",
    "        #mort_pred_up=conf_int_d[1]/conf_int_con[0]\n",
    "    \n",
    "    #fix province\n",
    "    province_name=''\n",
    "    if (pd.notnull(province[co])) :\n",
    "        province_name=province[co]\n",
    "    \n",
    "    if (ignore_recoveries==0):\n",
    "        R_value2=int(best_pred_recoveries2) \n",
    "        R_value7=int(best_pred_recoveries7) \n",
    "        R_value30=int(best_pred_recoveries30) \n",
    "        #low_R=int(conf_int_r[0])\n",
    "        #high_R=int(conf_int_r[1])\n",
    "    else:\n",
    "        R_value2=''\n",
    "        R_value7=''\n",
    "        R_value30=''\n",
    "        #low_R=''\n",
    "        #high_R=''\n",
    "        \n",
    "    #Critical Cases:\n",
    "    #Assume that 5% of all cases are critical and need hospitalization:\n",
    "    serious_percentage=0.05\n",
    "    critical2=serious_percentage*best_pred_confirmed2\n",
    "    critical7=serious_percentage*best_pred_confirmed7\n",
    "    critical30=serious_percentage*best_pred_confirmed30\n",
    "    #low_critical=serious_percentage*conf_int_con[0]\n",
    "    #high_critical=serious_percentage*conf_int_con[1]\n",
    "    \n",
    "    total_predictions2 = total_predictions2.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates2[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed2) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value2 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths2) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred2,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical2) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "        \n",
    "    total_predictions7 = total_predictions7.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates7[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed7) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value7 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths7) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred7,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical7) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "            \n",
    "    total_predictions30 = total_predictions30.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates30[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed30) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value30 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths30) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred30,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical30) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "    \n",
    "    if (k==1):\n",
    "        header = True\n",
    "    else:\n",
    "        header = False\n",
    "    \n",
    "    total_predictions2.to_csv(pred_file2, sep=',',index = False,  mode='a', header = header)\n",
    "    total_predictions7.to_csv(pred_file7, sep=',',index = False,  mode='a', header = header)\n",
    "    total_predictions30.to_csv(pred_file30, sep=',',index = False,  mode='a', header = header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
