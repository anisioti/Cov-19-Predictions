{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import operator \n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "from datetime import date\n",
    "import operator\n",
    "from matplotlib import pyplot\n",
    "#import rpy2 as r\n",
    "#from tslearn.utils import to_time_series_dataset\n",
    "#from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the files in the same folder with code\n",
    "confirmed_cases=pd.read_csv('confirmed_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_cases=pd.read_csv('deaths_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoveries_cases=pd.read_csv('recoveries_cases_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = confirmed_cases.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonames = recoveries_cases.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = confirmed_cases.loc[:, colnames[4]:colnames[-1]]\n",
    "deaths = deaths_cases.loc[:, colnames[4]:colnames[-1]]\n",
    "recoveries = recoveries_cases.loc[:, colonames[4]:colonames[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_date = confirmed.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = np.array([i for i in range(len(time_date))]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_days_ahead = 2\n",
    "#pred_days_ahead = 7\n",
    "#pred_days_ahead = 30\n",
    "#predictions_ahead = np.array([i for i in range(len(time_date)+pred_days_ahead)]).reshape(-1, 1)\n",
    "\n",
    "#predictions_ahead_plus_day = np.array([i for i in range(len(time_date)+pred_days_ahead+1)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_days_ahead = 30\n",
    "predictions_ahead = np.array([i for i in range(len(time_date)+pred_days_ahead)]).reshape(-1, 1)\n",
    "\n",
    "predictions_ahead_plus_day2 = np.array([i for i in range(len(time_date)+2+1)]).reshape(-1, 1)\n",
    "predictions_ahead_plus_day7 = np.array([i for i in range(len(time_date)+7+1)]).reshape(-1, 1)\n",
    "predictions_ahead_plus_day30 = np.array([i for i in range(len(time_date)+30+1)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = datetime.datetime.strptime(colnames[4], \"%m/%d/%y\")\n",
    "#end = datetime.datetime.strptime(colnames[-1], \"%m/%d/%y\")\n",
    "#predictions_ahead_dates = [start + datetime.timedelta(days=d) for d in range(len(predictions_ahead_plus_day))]\n",
    "\n",
    "start = datetime.datetime.strptime(colnames[4], \"%m/%d/%y\")\n",
    "end = datetime.datetime.strptime(colnames[-1], \"%m/%d/%y\")\n",
    "predictions_ahead_dates2 = [start + datetime.timedelta(days=d) for d in range(len(predictions_ahead_plus_day2))]\n",
    "predictions_ahead_dates7 = [start + datetime.timedelta(days=d) for d in range(len(predictions_ahead_plus_day7))]\n",
    "predictions_ahead_dates30 = [start + datetime.timedelta(days=d) for d in range(len(predictions_ahead_plus_day30))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_cases = confirmed_cases[~confirmed_cases['Country/Region'].isin(['Tajikistan', 'Comoros'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries=confirmed_cases['Country/Region'].tolist()\n",
    "province=confirmed_cases['Province/State'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country = countries[264:-1]\n",
    "#provinces = province[264:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_stationarity(timeseries):\n",
    "    \n",
    "#     #Determing rolling statistics\n",
    "#     rolmean = timeseries.rolling(12).mean()     \n",
    "#     rolstd = timeseries.rolling(12).std()\n",
    "    \n",
    "#     #Perform Dickey-Fuller test:\n",
    "#     dftest = adfuller(timeseries, autolag='AIC')\n",
    "#     dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "#     for key,value in dftest[4].items():\n",
    "#         dfoutput['Critical Value (%s)'%key] = value\n",
    "#     return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#deaths_cases.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(last_ob, value):\n",
    "    return value + last_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input timeseries as a dataframe\n",
    "# def make_stationary(timeseries):\n",
    "#     #remove zeroes to find the optimal d\n",
    "#     timeseries=timeseries[timeseries['cases'] != 0]\n",
    "#     #find the optimal d \n",
    "#     diff=timeseries-timeseries.shift(1)\n",
    "#     for d in range(1,10):\n",
    "#         if (test_stationarity(diff.dropna())['p-value']<0.1):\n",
    "#             break\n",
    "#         else: \n",
    "#             d=d+1 \n",
    "#             diff=diff-diff.shift(1)\n",
    "#     return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['diff'+str(i)]=df['diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takensEmbedding (data, delay, dimension):\n",
    "    \"This function returns the Takens embedding of data with delay into dimension, delay*dimension must be < len(data)\"\n",
    "    if delay*dimension > len(data):\n",
    "        raise NameError('Delay times dimension exceed length of data!')    \n",
    "    embeddedData = np.array([data[0:len(data)-delay*dimension]])\n",
    "    for i in range(1, dimension):\n",
    "        embeddedData = np.append(embeddedData, [data[i*delay:len(data) - delay*(dimension - i)]], axis=0)\n",
    "    return embeddedData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data (region, delay=1 , dimension=31):\n",
    "    #create a dataframe for timeseries analysis\n",
    "    date_rng = pd.date_range(start=colnames[4], end=colnames[-1], freq='D') #creates the days\n",
    "    df = pd.DataFrame(date_rng, columns=['date'])\n",
    "    df['cases']=np.transpose(region) #this is a dataframe with dates and cases.\n",
    "#     df = df.set_index('date')\n",
    "#     timeseries=df[df['cases'] != 0]\n",
    "#     #find the optimal d \n",
    "#     diff=timeseries-timeseries.shift(1)\n",
    "#     for d in range(1,10):\n",
    "#         if (test_stationarity(diff.dropna())['p-value']<0.1):\n",
    "#             break\n",
    "#         else: \n",
    "#             d=d+1 \n",
    "#             diff=diff-diff.shift(1)\n",
    "#     for i in range (1,d):\n",
    "#         i=i+1 \n",
    "#         diff=diff-diff.shift(1)\n",
    "#         df['diff'+str(i)]=diff\n",
    "    #df['diff']=diff #column of the dataframe with the stationary timeseries, might contain Na's!\n",
    "    df['diff']=df['cases']-df['cases'].shift(1)\n",
    "    t_diff=(df['diff'].dropna()).to_numpy() \n",
    "    timeseries_embedded=np.transpose(takensEmbedding(t_diff,delay,dimension))\n",
    "    Y_train=timeseries_embedded[:,-1]\n",
    "    X_train=timeseries_embedded[:,:-1] #keep all apart from the last column\n",
    "    X_test=t_diff[-(dimension-1):] #predict using as feature the last 'dimension' lags\n",
    "    return X_train , Y_train, X_test , t_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data(reg,1,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=prepare_data(reg,1,31)[0]\n",
    "# Y_train=prepare_data(reg,1,31)[1]\n",
    "# X_test=prepare_data(reg,1,31)[2]\n",
    "# t_diff=prepare_data(reg,1,31)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=t_diff[-30:]\n",
    "# X_test\n",
    "# pred_days_ahead=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "#     X_test=np.append(X_test[1:],predictions[i])\n",
    "#     #in every iteration remove the first element of X_test and add prediction\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together=np.append(t_diff,predictions)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together=np.cumsum(together)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,d):\n",
    "#         together=np.cumsum(together)\n",
    "# together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_ML(region, pred_days_ahead, delay=1,dimension=30):\n",
    "    predictions=np.zeros(pred_days_ahead)\n",
    "    X_train = prepare_data(region, delay, dimension)[0] \n",
    "    Y_train = prepare_data(region, delay, dimension)[1]\n",
    "    X_test = prepare_data(region, delay, dimension)[2]\n",
    "    for i in range(pred_days_ahead):\n",
    "        regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "        r=regressor.fit(X_train, Y_train)\n",
    "        predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "        X_test=np.append(X_test[1:],int(predictions[i])) \n",
    "        #in every iteration remove the first element of X_test and add prediction\n",
    "    predictions #we have to go back to the original scale\n",
    "    #d = prepare_data(region, delay, dimension)[3]\n",
    "    t_diff = prepare_data(region, delay, dimension)[3]\n",
    "    together=np.append(t_diff,predictions)\n",
    "    #for i in range(1,d):\n",
    "    together=np.cumsum(together)\n",
    "    return together #returns an array with all the cases and the predictions (we want the last elements of it)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=predict_with_ML(reg,30,1,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_days_ahead=30\n",
    "# x[-pred_days_ahead+6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = confirmed_cases.loc[(confirmed_cases['Country/Region'] == \"US\") & (pd.isnull(confirmed_cases['Province/State']))]\n",
    "# reg =  np.array(region.loc[:, colnames[4]:colnames[-1]])\n",
    "# len(reg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_rng = pd.date_range(start=colnames[4], end=colnames[-1], freq='D') #creates the days\n",
    "# df = pd.DataFrame(date_rng, columns=['date'])\n",
    "# df['cases']=np.transpose(reg) #this is a dataframe with dates and cases.\n",
    "# df = df.set_index('date')\n",
    "# #df=df[df['cases'] != 0] #remove zeroes, they do not help for analysis.\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['diff']=df['cases']-df['cases'].shift(1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def takensEmbedding (data, delay, dimension):\n",
    "#     \"This function returns the Takens embedding of data with delay into dimension, delay*dimension must be < len(data)\"\n",
    "#     if delay*dimension > len(data):\n",
    "#         raise NameError('Delay times dimension exceed length of data!')    \n",
    "#     embeddedData = np.array([data[0:len(data)-delay*dimension]])\n",
    "#     for i in range(1, dimension):\n",
    "#         embeddedData = np.append(embeddedData, [data[i*delay:len(data) - delay*(dimension - i)]], axis=0)\n",
    "#     return embeddedData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_diff=(df['diff'].dropna()).to_numpy() \n",
    "# np.shape(t_diff) #the function wants as input an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries_embedded=np.transpose(takensEmbedding(t_diff,1,11))\n",
    "# np.shape(timeseries_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train=timeseries_embedded[:,-1]\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=timeseries_embedded[:,:-1] #keep all apart from the last column\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.transpose(reg)[-30:].reshape(1,-1)\n",
    "#X_test=np.transpose(x)[0].reshape(1,-1)\n",
    "#X_test=np.transpose(t_diff[-30:]).reshape(1,-1)\n",
    "# X_test=t_diff[-10:]\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #regressor = RandomForestRegressor(n_estimators = 500, random_state = 42) \n",
    "# regressor =  GradientBoostingRegressor(n_estimators=500, learning_rate=0.1,\n",
    "#                                             max_depth=1, random_state=0, loss='huber')\n",
    "# #regressor = SVR(kernel='rbf', C=1 )\n",
    "# r=regressor.fit(X_train, Y_train)\n",
    "# predictions = regressor.predict(X_test.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "# #X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=np.append(X_test[1:],int(predictions))\n",
    "# #len(X_test)\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_days_ahead=2\n",
    "# X_train\n",
    "# #X_train[:-1,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test.reshape(1, -1))\n",
    "#     X_test=np.append(X_test[1:],int(predictions[i])) \n",
    "#     #in every iteration remove the first element of X_test and add prediction\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions=np.zeros(pred_days_ahead)\n",
    "# for i in range(pred_days_ahead):\n",
    "#     regressor = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#     r=regressor.fit(X_train, Y_train)\n",
    "#     predictions[i] = regressor.predict(X_test)\n",
    "#     Y_train = Y_train[:-1] #remove the last element of Y_train\n",
    "#     X_train=X_train[:-1,:] #remove the last row of X_train\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def subpopulation_Analysis(cases,country,province, all_days,subpopulation, pred_days_ahead=30):\n",
    "    \n",
    "    pred  = list()\n",
    "    if pd.isnull(province):\n",
    "        region = cases.loc[(cases['Country/Region'] == country) & (pd.isnull(cases['Province/State']))]\n",
    "    else:\n",
    "        print(\"Hi\")\n",
    "        print(country)\n",
    "        print(province)\n",
    "        region = cases.loc[(cases['Country/Region'] == country)& (cases['Province/State'] == province)]\n",
    "    print(\"Region\")\n",
    "    print(region)\n",
    "    \n",
    "    reg =  np.array(region.loc[:, colnames[4]:colnames[-1]])\n",
    "    region =  np.array(region.loc[:, colnames[4]:colnames[-1]]).reshape(-1, 1)\n",
    "        \n",
    "    #train_x, test_x, train_y, test_y = model_selection.train_test_split(all_days,region,test_size = 0.04,random_state = 42,shuffle=False)\n",
    "    \n",
    "    #pred_svm =  SVM_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "    #pred.append(pred_svm)\n",
    "    \n",
    "    #TODO: a for loop\n",
    "    daily_increase_threshold1=np.absolute((reg[-1][-1]-reg[-1][-2])/reg[-1][-2])*100\n",
    "    daily_increase_threshold2=np.absolute((reg[-1][-2]-reg[-1][-3])/reg[-1][-3])*100\n",
    "    daily_increase_threshold3=np.absolute((reg[-1][-3]-reg[-1][-4])/reg[-1][-4])*100\n",
    "    daily_increase_threshold4=np.absolute((reg[-1][-4]-reg[-1][-5])/reg[-1][-5])*100\n",
    "    daily_increase_bound=1000\n",
    "    daily_increase_threshold = max(daily_increase_threshold1,daily_increase_threshold2,daily_increase_threshold3,daily_increase_threshold4)\n",
    "    \n",
    "    if (daily_increase_threshold>daily_increase_bound):\n",
    "        best_pred_ahead2 = 0 \n",
    "        best_pred_ahead7 = 0 \n",
    "        best_pred_ahead30 = 0 \n",
    "        region = region\n",
    "        best_pred = 0\n",
    "        #best_conf_int = np.zeros(2)\n",
    "        reject_country = 1\n",
    "        return(best_pred_ahead2,best_pred_ahead7,best_pred_ahead30, region, best_pred, reject_country)\n",
    "     \n",
    "    \n",
    "    pred_arima=predict_with_ML(reg, pred_days_ahead, delay=1,dimension=31)\n",
    "    #pred.append(pred_svm)\n",
    "    #pred_RF = RF_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "    #pred.append(pred_RF)\n",
    "\n",
    "    \n",
    "   # pred_gb =  Grad_Boost_regression(train_x, train_y.ravel(), test_x, test_y, predictions_ahead,subpopulation,all_days,region.ravel())\n",
    "   # pred.append(pred_gb)\n",
    "\n",
    "\n",
    "    if (subpopulation == 'confirmed'): \n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_confirmed.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "    elif (subpopulation == 'deaths'):\n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_deaths.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "    else :\n",
    "        if (len(pred_arima)==0):\n",
    "            min_idx, (min_key, min_val) = min(enumerate(prediction_models_recoveries.items()), key=lambda x: x[1][1])\n",
    "            best_pred = pred[min_idx]\n",
    "            #conf_int=np.zeros(2)\n",
    "        else:\n",
    "            best_pred=pred_arima\n",
    "            #conf_int=conf_int\n",
    "        \n",
    "    #print(best_pred)\n",
    "    #print(type(best_pred))\n",
    "    best_pred_ahead2 = best_pred[-pred_days_ahead+1]\n",
    "    #print(\"gntm2\")\n",
    "    #print(best_pred_ahead2)\n",
    "    best_pred_ahead7= best_pred[-pred_days_ahead+6]\n",
    "    #print(\"gntm7\")\n",
    "    #print(best_pred_ahead7)\n",
    "    best_pred_ahead30= best_pred[-pred_days_ahead-1]\n",
    "    #print(\"gntm30\")\n",
    "    #print(best_pred_ahead30)\n",
    "    #best_conf_int=conf_int[-1]\n",
    "   \n",
    "    #best_conf_int[0]=max(reg[-1][-1], best_conf_int[0])\n",
    "    #if (best_conf_int[0]>best_conf_int[1]):\n",
    "        #best_conf_int[1]=2*reg[-1][-1]\n",
    "\n",
    "    if(best_pred_ahead2<reg[-1][-1]):\n",
    "        best_pred_ahead2= reg[-1][-1]+2\n",
    "    \n",
    "    if(best_pred_ahead7<reg[-1][-1]):\n",
    "        best_pred_ahead7= reg[-1][-1]+7   \n",
    "    \n",
    "    if(best_pred_ahead7<reg[-1][-1]):\n",
    "        best_pred_ahead7= reg[-1][-1]+30     \n",
    "    \n",
    "    reject_country = 0\n",
    "    \n",
    "    return best_pred_ahead2,best_pred_ahead7,best_pred_ahead30, region, best_pred, reject_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2day_prediction_random_forest2020-06-12.csv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file2 = str(2)+\"day_prediction_random_forest\" +  date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7day_prediction_random_forest2020-06-12.csv'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file7 = str(7)+\"day_prediction_random_forest\" + date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30day_prediction_random_forest2020-06-12.csv'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file30 = str(30)+\"day_prediction_random_forest\" + date.today().strftime('%Y-%m-%d') + \".csv\"\n",
    "pred_file30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "0        0        0        0  ...   16509   17267   18054   18969   19551   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "0   20342   20917   21459    22142    22890  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "0        0        0        0  ...     270     294     300     309     327   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "0     357     369     384      405      426  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "0            NaN    Afghanistan        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "0        0        0        0  ...    1450    1522    1585    1762    1830   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "0    1875    2171    2651     3013     3326  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "1            NaN        Albania        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "1        0        0        0  ...    1164    1184    1197    1212    1232   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "1    1246    1263    1299     1341     1385  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "1            NaN        Albania        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "1        0        0        0  ...      33      33      33      33      34   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "1      34      34      34       34       35  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "1            NaN        Albania        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "1        0        0        0  ...     891     898     898     910     925   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "1     938     945     960      980     1001  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "2            NaN        Algeria        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "2        0        0        0  ...    9626    9733    9831    9935   10050   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "2   10154   10265   10382    10484    10589  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "2            NaN        Algeria        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "2        0        0        0  ...     667     673     681     690     698   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "2     707     715     724      732      741  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "2            NaN        Algeria        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "2        0        0        0  ...    6067    6218    6297    6453    6631   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "2    6717    6799    6951     7074     7255  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "3            NaN        Andorra        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "3        0        0        0  ...     844     851     852     852     852   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "3     852     852     852      852      852  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "Region\n",
      "  Province/State Country/Region  1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  \\\n",
      "3            NaN        Andorra        0        0        0        0        0   \n",
      "\n",
      "   1/27/20  1/28/20  1/29/20  ...  6/2/20  6/3/20  6/4/20  6/5/20  6/6/20  \\\n",
      "3        0        0        0  ...      51      51      51      51      51   \n",
      "\n",
      "   6/7/20  6/8/20  6/9/20  6/10/20  6/11/20  \n",
      "3      51      51      51       51       51  \n",
      "\n",
      "[1 rows x 144 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['Province/State','Country','Target/Date','N','low95N','high95N',\n",
    "          'R','low95R','high95R','D','low95D','high95D','T','low95T','high95T',\n",
    "           'M','low95M','high95M','C','low95C','high95C']\n",
    "\n",
    "            \n",
    "\n",
    "k = 0\n",
    "\n",
    "for co in range(len(countries)):\n",
    "    \n",
    "    ignore_recoveries = 0    \n",
    "    \n",
    "    total_predictions2 = pd.DataFrame(columns = columns)\n",
    "    total_predictions7 = pd.DataFrame(columns = columns)\n",
    "    total_predictions30 = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "\n",
    "    best_pred_confirmed2,best_pred_confirmed7,best_pred_confirmed30, region_con, best_pred_con, reject_country = subpopulation_Analysis(confirmed_cases,countries[co],province[co],\n",
    "                                                 all_days,\"confirmed\",pred_days_ahead)\n",
    "    \n",
    "    if (reject_country==1):\n",
    "        print('rejected_conf')\n",
    "        continue\n",
    "        \n",
    "    best_pred_deaths2,best_pred_deaths7,best_pred_deaths30, region_d, best_pred_d, reject_country = subpopulation_Analysis(deaths_cases,countries[co],province[co],\n",
    "                                              all_days,\"deaths\",pred_days_ahead)\n",
    "    if (reject_country==1):\n",
    "        print('rejected_d')\n",
    "        continue\n",
    "        \n",
    "    if ((countries[co] == 'US' or countries[co] == 'Canada' ) & (pd.notnull(province[co]))):\n",
    "        ignore_recoveries=1   \n",
    "    else:\n",
    "        best_pred_recoveries2,best_pred_recoveries7,best_pred_recoveries30, region_r, best_pred_r, reject_country = subpopulation_Analysis(recoveries_cases,countries[co],province[co],\n",
    "                                                  all_days,\"recoveries\",pred_days_ahead)\n",
    "    \n",
    "    if (reject_country==1):\n",
    "        print('rejected_r')\n",
    "        continue\n",
    "    \n",
    "    # Mortality rate\n",
    "    #if (conf_int_con[0]==0):\n",
    "       # mort_pred=0\n",
    "        #mort_pred_low=0\n",
    "        #mort_pred_up=0\n",
    "    #else:\n",
    "    mort_pred2  = best_pred_deaths2/best_pred_confirmed2\n",
    "    mort_pred7  = best_pred_deaths7/best_pred_confirmed7\n",
    "    mort_pred30  = best_pred_deaths30/best_pred_confirmed30\n",
    "        #mort_pred_low= conf_int_d[0]/conf_int_con[1]\n",
    "        #mort_pred_up=conf_int_d[1]/conf_int_con[0]\n",
    "    \n",
    "    #fix province\n",
    "    province_name=''\n",
    "    if (pd.notnull(province[co])) :\n",
    "        province_name=province[co]\n",
    "    \n",
    "    if (ignore_recoveries==0):\n",
    "        R_value2=int(best_pred_recoveries2) \n",
    "        R_value7=int(best_pred_recoveries7) \n",
    "        R_value30=int(best_pred_recoveries30) \n",
    "        #low_R=int(conf_int_r[0])\n",
    "        #high_R=int(conf_int_r[1])\n",
    "    else:\n",
    "        R_value2=''\n",
    "        R_value7=''\n",
    "        R_value30=''\n",
    "        #low_R=''\n",
    "        #high_R=''\n",
    "        \n",
    "    #Critical Cases:\n",
    "    #Assume that 5% of all cases are critical and need hospitalization:\n",
    "    serious_percentage=0.05\n",
    "    critical2=serious_percentage*best_pred_confirmed2\n",
    "    critical7=serious_percentage*best_pred_confirmed7\n",
    "    critical30=serious_percentage*best_pred_confirmed30\n",
    "    #low_critical=serious_percentage*conf_int_con[0]\n",
    "    #high_critical=serious_percentage*conf_int_con[1]\n",
    "    \n",
    "    total_predictions2 = total_predictions2.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates2[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed2) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value2 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths2) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred2,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical2) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "        \n",
    "    total_predictions7 = total_predictions7.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates7[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed7) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value7 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths7) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred7,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical7) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "            \n",
    "    total_predictions30 = total_predictions30.append({'Province/State': province_name,\n",
    "                                                'Country': countries[co],\n",
    "                                                'Target/Date': predictions_ahead_dates30[-1].strftime('%Y-%m-%d'), \n",
    "                                                'N': int(best_pred_confirmed30) ,\n",
    "                                                'low95N':'',\n",
    "                                                'high95N': '',\n",
    "                                                'R': R_value30 ,\n",
    "                                                'low95R' : '',\n",
    "                                                'high95R' : '',\n",
    "                                                'D': int(best_pred_deaths30) ,\n",
    "                                                'low95D' : '',\n",
    "                                                'high95D': '',\n",
    "                                                'T': '',\n",
    "                                                'low95T' : '' ,\n",
    "                                                'high95T' : '',\n",
    "                                                'M': mort_pred30,\n",
    "                                                'low95M': '',\n",
    "                                                'high95M': '' ,\n",
    "                                                'C': int(critical30) ,\n",
    "                                                'low95C':  '' ,\n",
    "                                                'high95C': '' },ignore_index = True)\n",
    "    \n",
    "    if (k==1):\n",
    "        header = True\n",
    "    else:\n",
    "        header = False\n",
    "    \n",
    "    total_predictions2.to_csv(pred_file2, sep=',',index = False,  mode='a', header = header)\n",
    "    total_predictions7.to_csv(pred_file7, sep=',',index = False,  mode='a', header = header)\n",
    "    total_predictions30.to_csv(pred_file30, sep=',',index = False,  mode='a', header = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
